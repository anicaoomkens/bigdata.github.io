<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Personal Website</title>
    <meta name= "description" content="this is my website, I am Anica">
</head>
<link rel="stylesheet" href="stylesheet.css">
<style>
    body{
        width: 800px;
        margin: 0 auto;
    }
    img{
        width: 100%;
    }
</style>
<body>
    <div>
        <h1 style="color: rgb(114, 7, 7)">Big data</h1>
        <hr />
        <h2>Table of Contents</h2>
        <hr />
        <ul>
            <li> <a href="#sec1">Introduction</a> </li>
            <li> <a href="#sec2">Definition</a> </li>
            <li> <a href="#sec3">History</a> </li>
        </ul>
    </div>
    <img src="big data.jpg" alt="intro to big data">
    <p>Big data refers to data sets that are too large or complex to be dealt with by <a href="https://www.geeksforgeeks.org/difference-between-traditional-data-and-big-data/#:~:text=Traditional%20data%3A%20Traditional%20data%20is,or%20fields%20in%20a%20file.">traditional data-processing</a>  application software. <br> Data with many fields (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.[2] Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. <br> Big data was originally associated with three key concepts: <strong>volume, variety, and velocity.[3]</strong>  The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, then the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.[4]</p>
    <div>
        <h2 id="sec1">Introduction</h2>
        <hr />
        <p>Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require "massively parallel software running on tens, hundreds, or even thousands of servers".[19] What qualifies as "big data" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. "For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."[20]</p>
    </div>
    <div>
        <h2 id="sec2">Definition</h2>
        <hr />
        <p>The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.[21][22] Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.[23] Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data.[24] Big data "size" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.[25] Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.[26]</p>
    </div>
    <div>
        <h2 id="sec3">Histrory</h2>
        <hr />
        <p>In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc.[42] and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008.[43] In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.</p>
    </div>
</body>
</html>